{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a1f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 37_ngame-for-wikiseealsotitles-with-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b776411",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8936c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle, numpy as np\n",
    "from xcai.basics import *\n",
    "from xcai.models.LLL0XX import LAM009\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441a688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8906ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "os.environ['WANDB_PROJECT']='llama_00-wikiseealsotitles'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba30c3-7843-455c-b7be-7e7b9886dc4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9d4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98983822-d0cc-45ac-8241-ff3382999b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\n",
    "tokz.add_special_tokens({\"pad_token\": \"<PAD>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec3ad114",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = XCBlock.from_cfg(data_dir, 'data', transform_type='oak', tokenizer=tokz, metadata_name='lnk', num_labels=4, num_metadata=3,\n",
    "                         max_sequence_length=32, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00eddbb9-9c07-4223-beea-4a6f83f46ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_meta-llama-3-8b_oak.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ce105a1-b5be-4d0a-a75f-cc25e0404aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e695f233-d904-4c8a-8060-b17e45d91b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad847e4-0b9a-4706-beb7-0c887da9712f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a84c8fe7-fe8f-46d2-8bfd-de1c50333693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml version 3.39.1 is installed, but version 3.43.2 or higher is required. Please update comet_ml to the latest version to enable Comet logging with pip install 'comet-ml>=3.43.2'.\n"
     ]
    }
   ],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/medic/37_ngame-for-wikiseealsotitles-with-llama',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=300,\n",
    "    predict_with_representation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    adam_epsilon=1e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    group_by_cluster=True,\n",
    "    num_clustering_warmup_epochs=10,\n",
    "    num_cluster_update_epochs=5,\n",
    "    num_cluster_size_update_epochs=25,\n",
    "    clustering_type='EXPO',\n",
    "    minimum_cluster_size=2,\n",
    "    maximum_cluster_size=1600,\n",
    "    \n",
    "    metric_for_best_model='P@1',\n",
    "    load_best_model_at_end=True,\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    \n",
    "    use_encoder_parallel=True,\n",
    "    max_grad_norm=None,\n",
    "    fp16=True,\n",
    "\n",
    "    label_names=['lbl2data_idx', 'lbl2data_input_ids', 'lbl2data_attention_mask', 'data_idx'],\n",
    "\n",
    "    accelerator_config={\"use_configured_state\":True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d14de7e1-5e7a-4790-a170-e88112ca3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f61f4b-d4e9-49f3-95cb-c070f6b122b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a173b60c-3d15-40b5-9465-dd65bc544755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee1a64331f44f86a5c045e006b965ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LAM009 were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['model.encoder.dr_head.layer_norm.bias', 'model.encoder.dr_head.layer_norm.weight', 'model.encoder.dr_head.projector.bias', 'model.encoder.dr_head.projector.weight', 'model.encoder.dr_head.transform.bias', 'model.encoder.dr_head.transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = LAM009.from_pretrained('meta-llama/Meta-Llama-3-8B', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, n_negatives=10, \n",
    "                               apply_softmax=True, use_encoder_parallel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0009c3-8d25-4f68-ab8b-8de5751fd936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.init_retrieval_head()\n",
    "\n",
    "vocab_size = model.encoder.embed_tokens.num_embeddings\n",
    "model.encoder.resize_token_embeddings(vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59896b91-84a9-4302-9f73-7df5b8b9f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    bias='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5580e7e-75f5-432b-8417-0f564cc23f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7f4bb79-8737-40dc-b875-254827b336c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepresentationHead(\n",
       "  (transform): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
       "  (projector): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (activation): SiLU()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.encoder.dr_head.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a031b3-978d-4511-90ec-7113c224ed81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f193be-7e42-40a3-9de3-c4326933838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "deba5a8a-e183-4aef-b327-52be253bbcd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e2c894-aa11-4629-a156-56a98a709b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(block.test.dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3609898-94ec-41ab-9a70-cc0bcf8cbb5d",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5f9db-a09d-4d30-b790-c799f48c704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    build_block = False\n",
    "\n",
    "    data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "\n",
    "    \"\"\" Load data \"\"\"\n",
    "    pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_meta-llama-3-8b_oak.pkl'\n",
    "\n",
    "    if build_block:\n",
    "        from transformers import AutoTokenizer\n",
    "        tokz = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\n",
    "        tokz.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "\n",
    "        block = XCBlock.from_cfg(data_dir, 'data', transform_type='oak', tokenizer=tokz, metadata_name='lnk', num_labels=4, num_metadata=3,\n",
    "                                 max_sequence_length=32, padding=True, return_tensors='pt')\n",
    "\n",
    "        with open(pkl_file, 'wb') as file: pickle.dump(block, file)\n",
    "    else:\n",
    "        with open(pkl_file, 'rb') as file: block = pickle.load(file)\n",
    "\n",
    "\n",
    "    \"\"\" Training Arguements \"\"\"\n",
    "    args = XCLearningArguments(\n",
    "        output_dir='/home/scai/phd/aiz218323/scratch/outputs/medic/37_ngame-for-wikiseealsotitles-with-llama',\n",
    "        logging_first_step=True,\n",
    "        per_device_train_batch_size=800,\n",
    "        per_device_eval_batch_size=800,\n",
    "        representation_num_beams=200,\n",
    "        representation_accumulation_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=5000,\n",
    "        save_steps=5000,\n",
    "        save_total_limit=5,\n",
    "        num_train_epochs=300,\n",
    "        predict_with_representation=True,\n",
    "        representation_search_type='BRUTEFORCE',\n",
    "        adam_epsilon=1e-6,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=2e-4,\n",
    "        \n",
    "        group_by_cluster=True,\n",
    "        num_clustering_warmup_epochs=10,\n",
    "        num_cluster_update_epochs=5,\n",
    "        num_cluster_size_update_epochs=25,\n",
    "        clustering_type='EXPO',\n",
    "        minimum_cluster_size=2,\n",
    "        maximum_cluster_size=1600,\n",
    "        \n",
    "        metric_for_best_model='P@1',\n",
    "        load_best_model_at_end=True,\n",
    "        target_indices_key='plbl2data_idx',\n",
    "        target_pointer_key='plbl2data_data2ptr',\n",
    "        \n",
    "        use_encoder_parallel=True,\n",
    "        max_grad_norm=None,\n",
    "        fp16=True,\n",
    "\n",
    "        accelerator_config={\"use_configured_state\":True},\n",
    "    )\n",
    "\n",
    "    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "    model = LAM009.from_pretrained('meta-llama/Meta-Llama-3-8B', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, n_negatives=10, \n",
    "                                   apply_softmax=True, use_encoder_parallel=True)\n",
    "    \n",
    "    model.init_retrieval_head()\n",
    "    vocab_size = model.encoder.embed_tokens.num_embeddings\n",
    "    model.encoder.resize_token_embeddings(vocab_size+1)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias='none',\n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    peft_model.base_model.encoder.dr_head.requires_grad_(True)\n",
    "    \n",
    "    learn = XCLearner(\n",
    "        model=peft_model, \n",
    "        args=args,\n",
    "        train_dataset=block.train.dset,\n",
    "        eval_dataset=block.test.dset,\n",
    "        data_collator=block.collator,\n",
    "        compute_metrics=metric,\n",
    "    )\n",
    "    \n",
    "    mp.freeze_support()\n",
    "    learn.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a095df-0b9e-4f5e-80b1-a5b2372331fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3372c7-7bfe-436f-8d52-3b8a10f65e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-2',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=800,\n",
    "    per_device_eval_batch_size=800,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=100,\n",
    "    predict_with_representation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    use_encoder_parallel=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1438e-cdf1-4f2e-9991-f20e9aef027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875acda-5cee-45c1-b88e-e90273819396",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"/home/scai/phd/aiz218323/scratch/outputs/{os.path.basename(args.output_dir)}\"\n",
    "mname = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d74986-4c79-40d2-b984-0a7e4ccb9197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a2193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT009 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = DBT009.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1,\n",
    "                               n_negatives=10, apply_softmax=True, use_encoder_parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3e381-5e85-4411-99a0-54413275e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "model_weight_file = f'{mname}/model.safetensors'\n",
    "\n",
    "model_weights = {}\n",
    "with safe_open(model_weight_file, framework=\"pt\") as file:\n",
    "    for k in file.keys(): model_weights[k] = file.get_tensor(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1bd6b-1778-4805-910e-63e921ca950e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7813b43-75d1-45d9-a715-a726a6bee2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57a038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fee6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3968dd1cdc67413ea42d559d32ca42f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/scipy/sparse/_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "o = learn.predict(block.test.dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08e7e0-e5bc-4339-b124-f1af1d20df00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@3</th>\n",
       "      <th>N@5</th>\n",
       "      <th>N@10</th>\n",
       "      <th>PSP@1</th>\n",
       "      <th>PSP@3</th>\n",
       "      <th>PSP@5</th>\n",
       "      <th>PSP@10</th>\n",
       "      <th>PSN@1</th>\n",
       "      <th>PSN@3</th>\n",
       "      <th>PSN@5</th>\n",
       "      <th>PSN@10</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@100</th>\n",
       "      <th>R@200</th>\n",
       "      <th>loss</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.1771</td>\n",
       "      <td>16.1361</td>\n",
       "      <td>12.2884</td>\n",
       "      <td>7.9308</td>\n",
       "      <td>24.1771</td>\n",
       "      <td>24.4573</td>\n",
       "      <td>25.4609</td>\n",
       "      <td>27.2161</td>\n",
       "      <td>20.1098</td>\n",
       "      <td>21.5063</td>\n",
       "      <td>23.1904</td>\n",
       "      <td>26.8649</td>\n",
       "      <td>20.1098</td>\n",
       "      <td>22.0686</td>\n",
       "      <td>23.477</td>\n",
       "      <td>25.37</td>\n",
       "      <td>31.8421</td>\n",
       "      <td>46.4144</td>\n",
       "      <td>49.9468</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>262.9244</td>\n",
       "      <td>675.156</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P@1      P@3      P@5    P@10      N@1      N@3      N@5     N@10  \\\n",
       "0  24.1771  16.1361  12.2884  7.9308  24.1771  24.4573  25.4609  27.2161   \n",
       "\n",
       "     PSP@1    PSP@3    PSP@5   PSP@10    PSN@1    PSN@3   PSN@5  PSN@10  \\\n",
       "0  20.1098  21.5063  23.1904  26.8649  20.1098  22.0686  23.477   25.37   \n",
       "\n",
       "      R@10    R@100    R@200    loss   runtime  samples_per_second  \\\n",
       "0  31.8421  46.4144  49.9468  0.0294  262.9244             675.156   \n",
       "\n",
       "   steps_per_second  \n",
       "0             0.422  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metric(o.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436ccd3-e21c-454c-a24f-508f4afc2299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-2/checkpoint-85000'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bcf9a-c9c0-4da2-8f49-97163fd5c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6484280-d56f-43af-85a7-57c1a7e1c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = f\"{mname}/predictions/\"\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "with open(f'{pred_dir}/test_predictions.pkl', 'wb') as file: pickle.dump(o, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a35c47-8224-4117-8dc8-19de6dc840a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
