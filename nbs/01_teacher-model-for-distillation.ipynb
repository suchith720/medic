{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b51bd-0144-4139-8be5-7602bad6a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 01_teacher-model-for-distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e00e5c-ff88-425d-a828-7ca5d02215ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874750be-c904-447e-8754-3eefcb9586d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle, numpy as np\n",
    "from safetensors import safe_open\n",
    "from transformers import DistilBertConfig\n",
    "\n",
    "from xcai.basics import *\n",
    "from xcai.models.PPP0XX import DBT009\n",
    "from xcai.models.distillation import TCH001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44292259-cc09-4bd0-96f3-08206b948924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554994f-10ac-4d14-9e37-e1ec8a15826e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec27db-6509-41d6-b292-6637fb65d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-meta_distilbert-base-uncased_xcs_cat-128.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5bd41-8e37-457f-a36d-39e41a65ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as file: block = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ed91c-6774-4b4c-8d56-e873c96a826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d59040-ab99-4081-946e-d6a454da265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']\n",
    "block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']\n",
    "block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b607611-6648-454f-91c9-a02efa0cb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block.train.dset.meta = {}\n",
    "block.test.dset.meta = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb716ef-c80c-42ca-8096-5eed4ef7135b",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290555c3-bb42-41f7-bbc9-c92a6b24a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = XCLearningArguments(\n",
    "    output_dir='/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-4',\n",
    "    logging_first_step=True,\n",
    "    per_device_train_batch_size=800,\n",
    "    per_device_eval_batch_size=800,\n",
    "    representation_num_beams=200,\n",
    "    representation_accumulation_steps=100,\n",
    "    predict_with_representation=True,\n",
    "    representation_search_type='BRUTEFORCE',\n",
    "    target_indices_key='plbl2data_idx',\n",
    "    target_pointer_key='plbl2data_data2ptr',\n",
    "    use_encoder_parallel=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31d09f-4039-4b91-beb0-988f9509a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"/home/scai/phd/aiz218323/scratch/outputs/{os.path.basename(args.output_dir)}\"\n",
    "mname = f'{output_dir}/{os.path.basename(get_best_model(output_dir))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0fa0e-bb94-4c4f-82db-524027757773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_file,model_weights = f'{mname}/model.safetensors',{}\n",
    "with safe_open(model_weight_file, framework=\"pt\") as file:\n",
    "    for k in file.keys(): model_weights[k] = file.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfdf45-d7c6-4fb9-8702-ea9b3bbe34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DBT009 were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-v4 and are newly initialized: ['encoder.dr_layer_norm.bias', 'encoder.dr_layer_norm.weight', 'encoder.dr_projector.bias', 'encoder.dr_projector.weight', 'encoder.dr_transform.bias', 'encoder.dr_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "\n",
    "model = DBT009.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                               n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d2455-c8a2-46ee-bc0a-87ed3df6a456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6f4b7-c9ab-409a-bef3-d4e504e2e07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082c37d-5865-498f-8e37-ca0aecbd77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                  pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58524aa-2c4a-4ba4-97c5-5d05b4a4bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scai/phd/aiz218323/anaconda3/envs/xc_nlg/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "learn = XCLearner(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=block.train.dset,\n",
    "    eval_dataset=block.test.dset,\n",
    "    data_collator=block.collator,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a97f9-87b8-46c2-ae2c-1d8089b212bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda2fc1d7b9e4d44b6318270431e71f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a86367f0636462b9656cbae472e2c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_repr, lbl_repr = learn.get_data_and_lbl_representation(block.train.dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c96b0-83ed-4aab-9e22-41dc657f328b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0800a-6818-4116-a2aa-a5bf4be3fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCH001(DistilBertConfig(), n_data=block.train.dset.n_data, n_lbl=block.n_lbl)\n",
    "model.init_embeddings(data_repr, lbl_repr)\n",
    "\n",
    "model.save_pretrained(f'{output_dir}/teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d7919-43f9-4a73-bd0b-4430c18d8048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da203749-1839-4de0-a6b8-b9a0bc6d7f6f",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a8442-309a-4dbf-8a63-42095e93a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    show_metrics = True\n",
    "    dataset_type = 'wikiseealsotitles'\n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "    output_dir = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-4'\n",
    "    \n",
    "    \"\"\" Load data \"\"\"\n",
    "\n",
    "    if dataset_type == 'wikiseealsotitles':\n",
    "        pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-meta_distilbert-base-uncased_xcs_cat-128.pkl'\n",
    "        with open(pkl_file, 'rb') as file: block = pickle.load(file)\n",
    "    \n",
    "        \"\"\" Augment metadata \"\"\"\n",
    "        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']\n",
    "    \n",
    "        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']\n",
    "        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']\n",
    "        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "\n",
    "    elif dataset_type == 'wikititles':\n",
    "        pkl_file = f'{pkl_dir}/processed/wikititles_data-meta_distilbert-base-uncased_xcs_hlk-128.pkl'\n",
    "        with open(pkl_file, 'rb') as file: block = pickle.load(file)\n",
    "    \n",
    "        \"\"\" Augment metadata \"\"\"\n",
    "        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "    \n",
    "        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "\n",
    "    elif dataset_type == 'wikiseealso':\n",
    "        pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_xcs_cat-hlk-512.pkl'\n",
    "        with open(pkl_file, 'rb') as file: block = pickle.load(file)\n",
    "\n",
    "    elif dataset_type == 'wikipedia':\n",
    "        pkl_file = f'{pkl_dir}/processed/wikipedia_data-metas_distilbert-base-uncased_xcs-hlk-512.pkl'\n",
    "        with open(pkl_file, 'rb') as file: block = pickle.load(file)\n",
    "\n",
    "    block.train.dset.meta = {}\n",
    "    block.test.dset.meta = {}\n",
    "\n",
    "    \"\"\" Inference arguements \"\"\"\n",
    "    args = XCLearningArguments(\n",
    "        output_dir=output_dir,\n",
    "        logging_first_step=True,\n",
    "        per_device_train_batch_size=800,\n",
    "        per_device_eval_batch_size=800,\n",
    "        representation_num_beams=200,\n",
    "        representation_accumulation_steps=100,\n",
    "        predict_with_representation=True,\n",
    "        representation_search_type='BRUTEFORCE',\n",
    "        target_indices_key='plbl2data_idx',\n",
    "        target_pointer_key='plbl2data_data2ptr',\n",
    "        use_encoder_parallel=True,\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    \"\"\" Load model \"\"\"\n",
    "    mname = f'{args.output_dir}/{os.path.basename(get_best_model(args.output_dir))}'\n",
    "\n",
    "    model_weight_file,model_weights = f'{mname}/model.safetensors',{}\n",
    "    with safe_open(model_weight_file, framework=\"pt\") as file:\n",
    "        for k in file.keys(): model_weights[k] = file.get_tensor(k)\n",
    "\n",
    "    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()\n",
    "    model = DBT009.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, \n",
    "                                   n_negatives=10, apply_softmax=True, use_encoder_parallel=True)\n",
    "\n",
    "    model.load_state_dict(model_weights, strict=False)\n",
    "\n",
    "    \"\"\" Inference \"\"\"\n",
    "    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,\n",
    "                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])\n",
    "\n",
    "    learn = XCLearner(\n",
    "        model=model, \n",
    "        args=args,\n",
    "        train_dataset=block.train.dset,\n",
    "        eval_dataset=block.test.dset,\n",
    "        data_collator=block.collator,\n",
    "        compute_metrics=metric,\n",
    "    )\n",
    "\n",
    "    data_repr, lbl_repr = learn.get_data_and_lbl_representation(block.train.dset)\n",
    "\n",
    "    model = TCH001(DistilBertConfig(), n_data=block.train.dset.n_data, n_lbl=block.n_lbl)\n",
    "    model.init_embeddings(data_repr, lbl_repr)\n",
    "    model.save_pretrained(f'{output_dir}/teacher')\n",
    "\n",
    "    \"\"\" Metrics \"\"\"\n",
    "    if show_metrics:\n",
    "        o = learn.predict(block.test.dset)\n",
    "        print(o.metrics)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
