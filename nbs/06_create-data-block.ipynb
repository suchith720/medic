{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba54c3a-364f-48e2-b9e9-b20898d2e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 06_create-data-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3956df9-720c-439c-856b-3d1e6cb847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdf562-833d-475e-930d-2a4213858635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle, numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from xcai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e138a4-9ff2-4c9f-9056-eba05e6cd7b6",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa3a6f-4692-438b-9ea9-3a18cdb915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def separate_title_and_content(input_text):\n",
    "    title,content = [],[]\n",
    "    for o in tqdm(input_text):\n",
    "        text = o.split('  ', maxsplit=1)\n",
    "\n",
    "        if len(text) == 2: \n",
    "            t,c = text\n",
    "        else: \n",
    "            t,c = text[0], ''\n",
    "\n",
    "        title.append(t)\n",
    "        content.append(c)\n",
    "    return title, content\n",
    "\n",
    "def tokenize(tokenizer, info):\n",
    "    o = tokenizer(info['input_text'], truncation=True, max_length=32)\n",
    "    info.update(o)\n",
    "    \n",
    "    o = tokenizer(info['content_text'], truncation=True, max_length=1024)\n",
    "    \n",
    "    info['content_input_ids'] = o['input_ids']\n",
    "    info['content_attention_mask'] = o['attention_mask']\n",
    "\n",
    "def append_sep_token(input_ids, attention_mask):\n",
    "    sep_tok = input_ids[0][-1]\n",
    "    \n",
    "    new_input_ids = [o+[sep_tok] for o in input_ids]\n",
    "    new_attention_mask = [o+[1] for o in attention_mask]\n",
    "    \n",
    "    return new_input_ids, new_attention_mask\n",
    "\n",
    "def append_sep_token_into_block(block):\n",
    "    dset_type = ['train', 'test']\n",
    "    info_type = ['data_info', 'lbl_info']\n",
    "    \n",
    "    for dt in dset_type:\n",
    "        dset = getattr(block, dt)\n",
    "        for it in info_type:\n",
    "            info = getattr(dset.dset.data, it)\n",
    "            input_ids, attention_mask = append_sep_token(info['input_ids'], info['attention_mask'])\n",
    "            info['input_ids'],info['attention_mask'] = input_ids, attention_mask\n",
    "\n",
    "def concatenate_title_and_content_ids(info, max_input_length, exclude_sep_tok=True):\n",
    "    n_data = len(info['input_ids'])\n",
    "    sep_tok = info['input_ids'][0][-1]\n",
    "    \n",
    "    for i,(p,q) in tqdm(enumerate(zip(info['input_ids'],info['content_input_ids'])), total=n_data):\n",
    "        input_ids = p[:-1] + q[1:] if exclude_sep_tok else p + q[1:]\n",
    "        if len(input_ids) > max_input_length:\n",
    "            input_ids = input_ids[:max_input_length-1] + [sep_tok]\n",
    "        info['input_ids'][i] = input_ids\n",
    "    \n",
    "    for i,(p,q) in tqdm(enumerate(zip(info['attention_mask'],info['content_attention_mask'])), total=n_data):\n",
    "        attention_mask = p[:-1] + q[1:] if exclude_sep_tok else p + q[1:]\n",
    "        if len(attention_mask) > max_input_length:\n",
    "            attention_mask = attention_mask[:max_input_length-1] + [1]\n",
    "        info['attention_mask'][i] = attention_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a59646-5a20-44ed-9de4-fd12746612c5",
   "metadata": {},
   "source": [
    "## Data block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676f352-7b66-4716-ad6d-981eb5dcac3e",
   "metadata": {},
   "source": [
    "### `WIKISEEALSOTITLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df914b45-2061-47f1-86e0-5bbb10ea4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-lnk_distilbert-base-uncased_xcs.pkl'\n",
    "\n",
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "\n",
    "block = XCBlock.from_cfg(data_dir, 'data_lnk', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                         sampling_features=[('lbl2data',4), ('lnk2data',3)], oversample=True)\n",
    "\n",
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7943a-931a-43f8-b26f-c8fcd9286ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "    pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-meta_distilbert-base-uncased_xcs_cat-128.pkl'\n",
    "    \n",
    "    data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "    \n",
    "    block = XCBlock.from_cfg(data_dir, 'data_meta', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                             sampling_features=[('lbl2data',1)], oversample=False)\n",
    "    \n",
    "    \"\"\" Append metadata \"\"\"\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'data', 128, True)\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'lbl', 128, True)\n",
    "    \n",
    "    \n",
    "    with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d6aa6-1b54-44bc-937f-61450d962e3d",
   "metadata": {},
   "source": [
    "### `WIKITITLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b222b-63d0-4b53-96d5-06defb85bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "pkl_file = f'{pkl_dir}/processed/wikititles_data-lnk_distilbert-base-uncased_xcs.pkl'\n",
    "\n",
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "\n",
    "block = XCBlock.from_cfg(data_dir, 'data_lnk', dset='wikititles', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                         sampling_features=[('lbl2data',4), ('lnk2data',3)], oversample=True)\n",
    "\n",
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd01cd-6e53-49a9-aa03-df033af48f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'\n",
    "    pkl_file = f'{pkl_dir}/processed/wikititles_data-meta_distilbert-base-uncased_xcs_hlk-128.pkl'\n",
    "    \n",
    "    data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "    \n",
    "    block = XCBlock.from_cfg(data_dir, 'data_meta', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                             sampling_features=[('lbl2data',1)], oversample=False)\n",
    "    \n",
    "    \"\"\" Append metadata \"\"\"\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 128, True)\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 128, True)\n",
    "    \n",
    "    \n",
    "    with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f092ee9-a23e-4b56-948e-28379ac8d46a",
   "metadata": {},
   "source": [
    "### `WIKISEEALSO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9b6ca-c729-469c-a4b0-b5fb006f4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \"\"\" Load the data block \"\"\"\n",
    "    data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "    block = XCBlock.from_cfg(data_dir, 'data_metas', dset='wikiseealso', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                             sampling_features=[('lbl2data',1)], oversample=True)\n",
    "    \n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "    pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_xcs.pkl'\n",
    "    with open(pkl_file, 'wb') as file: pickle.dump(block, file)\n",
    "    \n",
    "    \"\"\" Separate title and content \"\"\"\n",
    "    train_title, train_content = separate_title_and_body(block.train.dset.data.data_info['input_text'])\n",
    "    test_title, test_content = separate_title_and_body(block.test.dset.data.data_info['input_text'])\n",
    "    \n",
    "    block.train.dset.data.data_info['input_text'] = train_title\n",
    "    block.train.dset.data.data_info['content_text'] = train_content\n",
    "    \n",
    "    block.test.dset.data.data_info['input_text'] = test_title\n",
    "    block.test.dset.data.data_info['content_text'] = test_content\n",
    "    \n",
    "    \"\"\" Tokenize the title and content \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    tokenize(tokenizer, block.train.dset.data.data_info)\n",
    "    tokenize(tokenizer, block.test.dset.data.data_info)\n",
    "    \n",
    "    append_sep_token_into_block(block)\n",
    "    \n",
    "    \"\"\" Append metadata \"\"\"\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'data', 128, True)\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'lbl', 128, True)\n",
    "    \n",
    "    block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "    block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "    block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "    block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']\n",
    "    \n",
    "    block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']\n",
    "    block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "    block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']\n",
    "    block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "    \n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 256, True)\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 256, True)\n",
    "    \n",
    "    block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "    block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "    block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "    block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "    \n",
    "    block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "    block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "    block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "    block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "    \n",
    "    \"\"\" Append the content \"\"\"\n",
    "    concatenate_title_and_content_ids(block.train.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "    concatenate_title_and_content_ids(block.test.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "    \n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "    pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_xcs_cat-hlk-512.pkl'\n",
    "    \n",
    "    with open(pkl_file, 'wb') as file: block = pickle.dump(block, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faac962-61c0-4659-b5ca-c0f992348dec",
   "metadata": {},
   "source": [
    "### `WIKIPEDIA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da29d41-75b4-452f-bd5d-8271c96b5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \"\"\" Load the data block \"\"\"\n",
    "    data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "    block = XCBlock.from_cfg(data_dir, 'data_metas', dset='wikipedia', transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                             sampling_features=[('lbl2data',1)], oversample=True)\n",
    "    \n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "    pkl_file = f'{pkl_dir}/processed/wikipedia_data-metas_distilbert-base-uncased_xcs.pkl'\n",
    "    with open(pkl_file, 'wb') as file: pickle.dump(block, file)\n",
    "    \n",
    "    \"\"\" Separate title and content \"\"\"\n",
    "    train_title, train_content = separate_title_and_body(block.train.dset.data.data_info['input_text'])\n",
    "    test_title, test_content = separate_title_and_body(block.test.dset.data.data_info['input_text'])\n",
    "    \n",
    "    block.train.dset.data.data_info['input_text'] = train_title\n",
    "    block.train.dset.data.data_info['content_text'] = train_content\n",
    "    \n",
    "    block.test.dset.data.data_info['input_text'] = test_title\n",
    "    block.test.dset.data.data_info['content_text'] = test_content\n",
    "    \n",
    "    \"\"\" Tokenize the title and content \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    tokenize(tokenizer, block.train.dset.data.data_info)\n",
    "    tokenize(tokenizer, block.test.dset.data.data_info)\n",
    "    \n",
    "    append_sep_token_into_block(block)\n",
    "    \n",
    "    \"\"\" Append metadata \"\"\"\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 256, True)\n",
    "    block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 256, True)\n",
    "    \n",
    "    block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "    block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "    block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "    block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "    \n",
    "    block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "    block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "    block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "    block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "    \n",
    "    \"\"\" Append the content \"\"\"\n",
    "    concatenate_title_and_content_ids(block.train.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "    concatenate_title_and_content_ids(block.test.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "    \n",
    "    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'\n",
    "    pkl_file = f'{pkl_dir}/processed/wikipedia_data-metas_distilbert-base-uncased_xcs_hlk-512.pkl'\n",
    "    \n",
    "    with open(pkl_file, 'wb') as file: block = pickle.dump(block, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ca708-b94f-4696-8907-9920c454f381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f17ee-0b1a-46d9-b12a-a9194defa3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b3b76-b3da-4d4f-93cb-f3fa885c965b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
