{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba54c3a-364f-48e2-b9e9-b20898d2e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp 06_create-data-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3956df9-720c-439c-856b-3d1e6cb847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdf562-833d-475e-930d-2a4213858635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os,torch, torch.multiprocessing as mp, pickle, numpy as np, argparse\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from xcai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e138a4-9ff2-4c9f-9056-eba05e6cd7b6",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa3a6f-4692-438b-9ea9-3a18cdb915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def separate_title_and_content(input_text):\n",
    "    title,content = [],[]\n",
    "    for o in tqdm(input_text):\n",
    "        text = o.split('  ', maxsplit=1)\n",
    "\n",
    "        if len(text) == 2: \n",
    "            t,c = text\n",
    "        else: \n",
    "            t,c = text[0], ''\n",
    "\n",
    "        title.append(t)\n",
    "        content.append(c)\n",
    "    return title, content\n",
    "\n",
    "def tokenize(tokenizer, info):\n",
    "    o = tokenizer(info['input_text'], truncation=True, max_length=32)\n",
    "    info.update(o)\n",
    "    \n",
    "    o = tokenizer(info['content_text'], truncation=True, max_length=1024)\n",
    "    \n",
    "    info['content_input_ids'] = o['input_ids']\n",
    "    info['content_attention_mask'] = o['attention_mask']\n",
    "\n",
    "def append_sep_token(input_ids, attention_mask):\n",
    "    sep_tok = input_ids[0][-1]\n",
    "    \n",
    "    new_input_ids = [o+[sep_tok] for o in input_ids]\n",
    "    new_attention_mask = [o+[1] for o in attention_mask]\n",
    "    \n",
    "    return new_input_ids, new_attention_mask\n",
    "\n",
    "def append_sep_token_into_block(block):\n",
    "    dset_type = ['train', 'test']\n",
    "    info_type = ['data_info', 'lbl_info']\n",
    "    \n",
    "    for dt in dset_type:\n",
    "        dset = getattr(block, dt)\n",
    "        for it in info_type:\n",
    "            info = getattr(dset.dset.data, it)\n",
    "            input_ids, attention_mask = append_sep_token(info['input_ids'], info['attention_mask'])\n",
    "            info['input_ids'],info['attention_mask'] = input_ids, attention_mask\n",
    "\n",
    "def concatenate_title_and_content_ids(info, max_input_length, exclude_sep_tok=True):\n",
    "    n_data = len(info['input_ids'])\n",
    "    sep_tok = info['input_ids'][0][-1]\n",
    "    \n",
    "    for i,(p,q) in tqdm(enumerate(zip(info['input_ids'],info['content_input_ids'])), total=n_data):\n",
    "        input_ids = p[:-1] + q[1:] if exclude_sep_tok else p + q[1:]\n",
    "        if len(input_ids) > max_input_length:\n",
    "            input_ids = input_ids[:max_input_length-1] + [sep_tok]\n",
    "        info['input_ids'][i] = input_ids\n",
    "    \n",
    "    for i,(p,q) in tqdm(enumerate(zip(info['attention_mask'],info['content_attention_mask'])), total=n_data):\n",
    "        attention_mask = p[:-1] + q[1:] if exclude_sep_tok else p + q[1:]\n",
    "        if len(attention_mask) > max_input_length:\n",
    "            attention_mask = attention_mask[:max_input_length-1] + [1]\n",
    "        info['attention_mask'][i] = attention_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dce27a-ca86-4706-bba8-0a199084fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_block(data_dir:str, dataset_name:str, block_type:str, pkl_dir:str, sampling_features=[('lbl2data',4), ('lnk2data',3)], \n",
    "                 oversample:Optional[bool]=False, pkl_suffix:Optional[str]=''):\n",
    "    pkl_suffix = f'-{pkl_suffix}' if pkl_suffix else pkl_suffix\n",
    "    pkl_file = f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs{pkl_suffix}.pkl'\n",
    "    block = XCBlock.from_cfg(data_dir, block_type, dset=dataset_name, transform_type='xcs', tokenizer='distilbert-base-uncased', \n",
    "                             sampling_features=sampling_features, oversample=oversample)\n",
    "    with open(pkl_file, 'wb') as file: pickle.dump(block, file)\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a59646-5a20-44ed-9de4-fd12746612c5",
   "metadata": {},
   "source": [
    "## Data block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc9b9f-9611-4c3f-b9f5-ed1755bec337",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676f352-7b66-4716-ad6d-981eb5dcac3e",
   "metadata": {},
   "source": [
    "### `WIKISEEALSOTITLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a0747-6e05-453c-a3ef-83cd8d9e8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikiseealsotitles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df914b45-2061-47f1-86e0-5bbb10ea4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_lnk'\n",
    "sampling_features = [('lbl2data',4), ('lnk2data',3)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7943a-931a-43f8-b26f-c8fcd9286ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_meta'\n",
    "sampling_features = [('lbl2data',1)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "\n",
    "\"\"\" Append metadata \"\"\"\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'data', 128, True)\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'lbl', 128, True)\n",
    "\n",
    "pkl_file = f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_cat-128.pkl'\n",
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d6aa6-1b54-44bc-937f-61450d962e3d",
   "metadata": {},
   "source": [
    "### `WIKITITLES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12bea1-3af3-4feb-ae83-d21ac7ed5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikititles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b222b-63d0-4b53-96d5-06defb85bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_lnk'\n",
    "sampling_features = [('lbl2data',4), ('lnk2data',3)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd01cd-6e53-49a9-aa03-df033af48f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_meta'\n",
    "sampling_features = [('lbl2data',1)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "\n",
    "\"\"\" Append metadata \"\"\"\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 128, True)\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 128, True)\n",
    "\n",
    "pkl_file = f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_hlk-128.pkl'\n",
    "with open(pkl_file, 'wb') as file: pickle.dump(block, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f092ee9-a23e-4b56-948e-28379ac8d46a",
   "metadata": {},
   "source": [
    "### `WIKISEEALSO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695f75b-3859-437e-bb67-d285eac1597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikiseealso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1590b6-9d30-452b-9677-5e2353fdb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_lnk'\n",
    "sampling_features = [('lbl2data',4), ('lnk2data',3)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9b6ca-c729-469c-a4b0-b5fb006f4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_metas'\n",
    "sampling_features = [('lbl2data',1)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "\n",
    "\"\"\" Separate title and content \"\"\"\n",
    "train_title, train_content = separate_title_and_body(block.train.dset.data.data_info['input_text'])\n",
    "test_title, test_content = separate_title_and_body(block.test.dset.data.data_info['input_text'])\n",
    "\n",
    "block.train.dset.data.data_info['input_text'] = train_title\n",
    "block.train.dset.data.data_info['content_text'] = train_content\n",
    "\n",
    "block.test.dset.data.data_info['input_text'] = test_title\n",
    "block.test.dset.data.data_info['content_text'] = test_content\n",
    "\n",
    "\"\"\" Tokenize the title and content \"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenize(tokenizer, block.train.dset.data.data_info)\n",
    "tokenize(tokenizer, block.test.dset.data.data_info)\n",
    "\n",
    "append_sep_token_into_block(block)\n",
    "\n",
    "\"\"\" Append metadata \"\"\"\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'data', 128, True)\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'lbl', 128, True)\n",
    "\n",
    "block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']\n",
    "\n",
    "block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']\n",
    "block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']\n",
    "block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 256, True)\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 256, True)\n",
    "\n",
    "block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "\n",
    "block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "\n",
    "\"\"\" Append the content \"\"\"\n",
    "concatenate_title_and_content_ids(block.train.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "concatenate_title_and_content_ids(block.test.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "\n",
    "pkl_file = f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_cat-hlk-512.pkl'\n",
    "with open(pkl_file, 'wb') as file: block = pickle.dump(block, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faac962-61c0-4659-b5ca-c0f992348dec",
   "metadata": {},
   "source": [
    "### `WIKIPEDIA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143935b-2da8-4a35-bd3f-0f737c5c3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikipedia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b61de0-5c36-4331-a35e-293bd378439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_lnk'\n",
    "sampling_features = [('lbl2data',4), ('lnk2data',3)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da29d41-75b4-452f-bd5d-8271c96b5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_type = 'data_metas'\n",
    "sampling_features = [('lbl2data',1)]\n",
    "oversample = False\n",
    "\n",
    "block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "    \n",
    "\"\"\" Separate title and content \"\"\"\n",
    "train_title, train_content = separate_title_and_body(block.train.dset.data.data_info['input_text'])\n",
    "test_title, test_content = separate_title_and_body(block.test.dset.data.data_info['input_text'])\n",
    "\n",
    "block.train.dset.data.data_info['input_text'] = train_title\n",
    "block.train.dset.data.data_info['content_text'] = train_content\n",
    "\n",
    "block.test.dset.data.data_info['input_text'] = test_title\n",
    "block.test.dset.data.data_info['content_text'] = test_content\n",
    "\n",
    "\"\"\" Tokenize the title and content \"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenize(tokenizer, block.train.dset.data.data_info)\n",
    "tokenize(tokenizer, block.test.dset.data.data_info)\n",
    "\n",
    "append_sep_token_into_block(block)\n",
    "\n",
    "\"\"\" Append metadata \"\"\"\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 256, True)\n",
    "block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 256, True)\n",
    "\n",
    "block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "\n",
    "block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "\n",
    "\"\"\" Append the content \"\"\"\n",
    "concatenate_title_and_content_ids(block.train.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "concatenate_title_and_content_ids(block.test.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "\n",
    "pkl_file = f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_hlk-512.pkl'\n",
    "with open(pkl_file, 'wb') as file: block = pickle.dump(block, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89a3f1-2ced-452f-9f47-645d45662f97",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f17ee-0b1a-46d9-b12a-a9194defa3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "experiment_type = 'linker'\n",
    "dataset_name = 'wikiseealsotitles'\n",
    "\n",
    "data_dir = '/home/scai/phd/aiz218323/Projects/XC_NLG/data'\n",
    "pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b3b76-b3da-4d4f-93cb-f3fa885c965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='create data block.')\n",
    "    parser.add_argument('--experiment_type', type=str, required=True)\n",
    "    parser.add_argument('--dataset_name', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    experiment_type, dataset_name = args.experiment_type, args.dataset_name\n",
    "    \n",
    "    if experiment_type == 'linker':\n",
    "        \n",
    "        block_type = 'data_lnk'\n",
    "        sampling_features = [('lbl2data',4), ('lnk2data',3)]\n",
    "        oversample = False\n",
    "        block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "    \n",
    "    elif experiment_type == 'teacher':\n",
    "    \n",
    "        if dataset_name == 'wikiseealsotitles' or dataset_name == 'wikititles':\n",
    "            block_type = 'data_meta'\n",
    "            sampling_features = [('lbl2data',1)]\n",
    "            oversample = False\n",
    "            \n",
    "            block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "            \n",
    "            \"\"\" Append metadata \"\"\"\n",
    "            meta_name = 'cat' if dataset_name == 'wikiseealsotitles' else 'hlk'\n",
    "            \n",
    "            block = AugmentMetaInputIdsTfm.apply(block, f'{meta_name}_meta', 'data', 128, True)\n",
    "            block = AugmentMetaInputIdsTfm.apply(block, f'{meta_name}_meta', 'lbl', 128, True)\n",
    "            \n",
    "            pkl_file = f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_{meta_name}-128.pkl'\n",
    "            with open(pkl_file, 'wb') as file: pickle.dump(block, file)\n",
    "            \n",
    "        elif dataset_name == 'wikiseealso' or dataset_name == 'wikipedia':\n",
    "            block_type = 'data_metas'\n",
    "            sampling_features = [('lbl2data',1)]\n",
    "            oversample = False\n",
    "            \n",
    "            block = create_block(data_dir, dataset_name, block_type, pkl_dir, sampling_features, oversample)\n",
    "            \n",
    "            \"\"\" Separate title and content \"\"\"\n",
    "            train_title, train_content = separate_title_and_body(block.train.dset.data.data_info['input_text'])\n",
    "            test_title, test_content = separate_title_and_body(block.test.dset.data.data_info['input_text'])\n",
    "            \n",
    "            block.train.dset.data.data_info['input_text'] = train_title\n",
    "            block.train.dset.data.data_info['content_text'] = train_content\n",
    "            \n",
    "            block.test.dset.data.data_info['input_text'] = test_title\n",
    "            block.test.dset.data.data_info['content_text'] = test_content\n",
    "            \n",
    "            \"\"\" Tokenize the title and content \"\"\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            tokenize(tokenizer, block.train.dset.data.data_info)\n",
    "            tokenize(tokenizer, block.test.dset.data.data_info)\n",
    "            \n",
    "            append_sep_token_into_block(block)\n",
    "    \n",
    "            \"\"\" Append metadata \"\"\"\n",
    "            if dataset_name == 'wikiseealso':\n",
    "                block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'data', 128, True)\n",
    "                block = AugmentMetaInputIdsTfm.apply(block, 'cat_meta', 'lbl', 128, True)\n",
    "                \n",
    "                block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']\n",
    "                block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']\n",
    "                block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']\n",
    "                block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']\n",
    "                \n",
    "                block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']\n",
    "                block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "                block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']\n",
    "                block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']\n",
    "                \n",
    "                block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 256, True)\n",
    "                block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 256, True)\n",
    "                \n",
    "                block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "                block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "                block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "                block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "                \n",
    "                block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "                block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "                block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "                block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "            else:\n",
    "                block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'data', 256, True)\n",
    "                block = AugmentMetaInputIdsTfm.apply(block, 'hlk_meta', 'lbl', 256, True)\n",
    "                \n",
    "                block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']\n",
    "                block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']\n",
    "                block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']\n",
    "                block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']\n",
    "                \n",
    "                block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "                block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "                block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']\n",
    "                block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']\n",
    "                \n",
    "            \"\"\" Append the content \"\"\"\n",
    "            concatenate_title_and_content_ids(block.train.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "            concatenate_title_and_content_ids(block.test.dset.data.data_info, 512, exclude_sep_tok=False)\n",
    "            \n",
    "            pkl_file = (\n",
    "                f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_hlk-512.pkl'\n",
    "                if dataset_name == 'wikipedia' else\n",
    "                f'{pkl_dir}/processed/{dataset_name}_{block_type.replace(\"_\", \"-\")}_distilbert-base-uncased_xcs_cat-hlk-512.pkl'\n",
    "            )\n",
    "            with open(pkl_file, 'wb') as file: block = pickle.dump(block, file)\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f'Invalid `dataset_name`: {dataset_name}')\n",
    "    else:\n",
    "        raise ValueError(f'Invalid `experiment_type`: {experiment_type}')\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
