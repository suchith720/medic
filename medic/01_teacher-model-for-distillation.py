# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_teacher-model-for-distillation.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/01_teacher-model-for-distillation.ipynb 2
import os,torch, torch.multiprocessing as mp, pickle, numpy as np
from safetensors import safe_open
from transformers import DistilBertConfig

from xcai.basics import *
from xcai.models.PPP0XX import DBT009
from xcai.models.distillation import TCH001

# %% ../nbs/01_teacher-model-for-distillation.ipynb 3
os.environ['WANDB_MODE'] = 'disabled'
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'

# %% ../nbs/01_teacher-model-for-distillation.ipynb 24
if __name__ == "__main__":
    show_metrics = True
    dataset_type = 'wikiseealsotitles'
    pkl_dir = '/home/scai/phd/aiz218323/scratch/datasets'
    output_dir = '/home/scai/phd/aiz218323/scratch/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-1-4'
    
    """ Load data """

    if dataset_type == 'wikiseealsotitles':
        pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-meta_distilbert-base-uncased_xcs_cat-128.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)
    
        """ Augment metadata """
        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']
        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']
        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']
        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']
    
        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']
        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']
        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']
        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']

    elif dataset_type == 'wikititles':
        pkl_file = f'{pkl_dir}/processed/wikititles_data-meta_distilbert-base-uncased_xcs_hlk-128.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)
    
        """ Augment metadata """
        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']
        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']
        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']
        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']
    
        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']
        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']
        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']
        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']

    elif dataset_type == 'amazontitles131':
        pkl_file = f'{pkl_dir}/processed/amazontitles131_data-meta_distilbert-base-uncased_xcs_cat-128.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']
        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']
        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']
        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']
    
        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']
        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']
        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']
        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']

    elif dataset_type == 'wikiseealso':
        pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_xcs_cat-hlk-512.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

    elif dataset_type == 'wikipedia':
        pkl_file = f'{pkl_dir}/processed/wikipedia_data-metas_distilbert-base-uncased_xcs-hlk-512.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

    elif dataset_type == 'amazon131':
        pkl_file = f'{pkl_dir}/processed/amazon131_data-meta_distilbert-base-uncased_xcs_cat-512.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

    block.train.dset.meta = {}
    block.test.dset.meta = {}

    """ Inference arguements """
    args = XCLearningArguments(
        output_dir=output_dir,
        logging_first_step=True,
        per_device_train_batch_size=800,
        per_device_eval_batch_size=800,
        representation_num_beams=200,
        representation_accumulation_steps=100,
        predict_with_representation=True,
        representation_search_type='BRUTEFORCE',
        target_indices_key='plbl2data_idx',
        target_pointer_key='plbl2data_data2ptr',
        use_encoder_parallel=True,
        fp16=True,
    )

    """ Load model """
    mname = f'{args.output_dir}/{os.path.basename(get_best_model(args.output_dir))}'

    model_weight_file,model_weights = f'{mname}/model.safetensors',{}
    with safe_open(model_weight_file, framework="pt") as file:
        for k in file.keys(): model_weights[k] = file.get_tensor(k)

    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()
    model = DBT009.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, 
                                   n_negatives=10, apply_softmax=True, use_encoder_parallel=True)

    model.load_state_dict(model_weights, strict=False)

    """ Inference """
    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])

    learn = XCLearner(
        model=model, 
        args=args,
        train_dataset=block.train.dset,
        eval_dataset=block.test.dset,
        data_collator=block.collator,
        compute_metrics=metric,
    )

    data_repr, lbl_repr = learn.get_data_and_lbl_representation(block.train.dset)

    model = TCH001(DistilBertConfig(), n_data=block.train.dset.n_data, n_lbl=block.n_lbl)
    model.init_embeddings(data_repr, lbl_repr)
    model.save_pretrained(f'{output_dir}/teacher')

    """ Metrics """
    if show_metrics:
        o = learn.predict(block.test.dset)
        print(o.metrics)
    
