# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_teacher-model-for-distillation.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/01_teacher-model-for-distillation.ipynb 2
import os,torch, torch.multiprocessing as mp, pickle, numpy as np
from safetensors import safe_open
from transformers import DistilBertConfig

from xcai.basics import *
from xcai.models.PPP0XX import DBT009
from xcai.models.distillation import TCH001

# %% ../nbs/01_teacher-model-for-distillation.ipynb 3
os.environ['WANDB_MODE'] = 'disabled'
os.environ['CUDA_VISIBLE_DEVICES'] = '12,13'

# %% ../nbs/01_teacher-model-for-distillation.ipynb 24
if __name__ == "__main__":
    show_metrics = True
    save_teacher = False
    save_predictions = False

    # dataset_type = 'wikititles'
    # output_dir = '/data/Projects/xc_nlg/outputs/93-ngame-ep-for-wikititles-with-input-concatenation-5-0/'

    dataset_type = 'wikiseealsotitles'
    output_dir = '/data/Projects/xc_nlg/outputs/67-ngame-ep-for-wikiseealso-with-input-concatenation-6-3/'

    # dataset_type = 'wikiseealso'
    # output_dir = '/home/aiscuser/scratch1/outputs/medic/11_ngame-for-wikiseealso-with-input-concatenation-001/'
    # output_dir = '/data/Projects/xc_nlg/outputs/94-ngame-ep-for-wikiseealso-with-input-concatenation-5-1/'

    pkl_dir = '/home/aiscuser/scratch1/datasets/'
    
    """ Load data """

    if dataset_type == 'wikiseealsotitles':
        pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data-meta_distilbert-base-uncased_xcs_cat-128.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)
    
        """ Augment metadata """
        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_cat']
        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_cat']
        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_cat']
        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_cat']
    
        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_cat']
        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_cat']
        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_cat']
        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_cat']

    elif dataset_type == 'wikititles':
        pkl_file = f'{pkl_dir}/processed/wikititles_data-meta_distilbert-base-uncased_xcs_hlk-128.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)
    
        """ Augment metadata """
        block.train.dset.data.data_info['input_ids'] = block.train.dset.data.data_info['input_ids_aug_hlk']
        block.train.dset.data.data_info['attention_mask'] = block.train.dset.data.data_info['attention_mask_aug_hlk']
        block.test.dset.data.data_info['input_ids'] = block.test.dset.data.data_info['input_ids_aug_hlk']
        block.test.dset.data.data_info['attention_mask'] = block.test.dset.data.data_info['attention_mask_aug_hlk']
    
        block.train.dset.data.lbl_info['input_ids'] = block.train.dset.data.lbl_info['input_ids_aug_hlk']
        block.train.dset.data.lbl_info['attention_mask'] = block.train.dset.data.lbl_info['attention_mask_aug_hlk']
        block.test.dset.data.lbl_info['input_ids'] = block.test.dset.data.lbl_info['input_ids_aug_hlk']
        block.test.dset.data.lbl_info['attention_mask'] = block.test.dset.data.lbl_info['attention_mask_aug_hlk']

    elif dataset_type == 'wikiseealso':
        pkl_file = f'{pkl_dir}/processed/wikiseealso_data-metas_distilbert-base-uncased_xcs_cat-hlk-256.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

    elif dataset_type == 'wikipedia':
        pkl_file = f'{pkl_dir}/processed/wikipedia_data-metas_distilbert-base-uncased_xcs-hlk-256.pkl'
        with open(pkl_file, 'rb') as file: block = pickle.load(file)

    block.train.dset.meta = {}
    block.test.dset.meta = {}

    """ Inference arguements """
    args = XCLearningArguments(
        output_dir=output_dir,
        logging_first_step=True,
        per_device_train_batch_size=800,
        per_device_eval_batch_size=800,
        representation_num_beams=200,
        representation_accumulation_steps=100,
        predict_with_representation=True,
        representation_search_type='BRUTEFORCE',
        target_indices_key='plbl2data_idx',
        target_pointer_key='plbl2data_data2ptr',
        use_encoder_parallel=True,
        fp16=True,
    )

    """ Load model """
    mname = f'{args.output_dir}/{os.path.basename(get_best_model(args.output_dir))}'

    model_weight_file,model_weights = f'{mname}/model.safetensors',{}
    with safe_open(model_weight_file, framework="pt") as file:
        for k in file.keys(): model_weights[k] = file.get_tensor(k)

    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()
    model = DBT009.from_pretrained('sentence-transformers/msmarco-distilbert-base-v4', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, 
                                   n_negatives=10, apply_softmax=True, use_encoder_parallel=True)

    model.load_state_dict(model_weights, strict=False)

    """ Inference """
    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])

    """
    pred_file = f'{output_dir}/predictions/test_predictions.pkl'
    with open(pred_file, 'rb') as file: o = pickle.load(file)

    indptr = block.test.dset.data.data_lbl.indptr
    output = { 
            'pred_idx': o.pred_idx, 
            'pred_ptr': o.pred_ptr, 
            'pred_score': o.pred_score, 
            'targ_idx': torch.from_numpy(block.test.dset.data.data_lbl.indices), 
            'targ_ptr': torch.from_numpy(np.stack([indptr[i+1]-indptr[i] for i in range(len(indptr)-1)])),
            }
    print(metric(**output))
    exit()
    """

    learn = XCLearner(
        model=model, 
        args=args,
        train_dataset=block.train.dset,
        eval_dataset=block.test.dset,
        data_collator=block.collator,
        compute_metrics=metric,
    )


    """
    from torch.utils.data import DataLoader
    from tqdm import tqdm
    def get_predictions(data_embed, lbl_embed):
        pred_idx, pred_score = [], []
        for batch in tqdm(DataLoader(data_embed, batch_size=4096)):
            score, idx = torch.topk(batch@lbl_embed.T, 200, dim=1)
            pred_idx.append(idx.flatten())
            pred_score.append(score.flatten())
        pred_idx, pred_score = torch.hstack(pred_idx), torch.hstack(pred_score)
        pred_ptr = torch.full((data_embed.shape[0],), 200)
        return pred_idx, pred_ptr, pred_score

    metric = PrecRecl(block.n_lbl, block.train.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])
    """
    
    if save_teacher:
        data_repr, lbl_repr = learn.get_data_and_lbl_representation(block.train.dset)

        # model = TCH001.from_pretrained(f'{output_dir}/teacher', n_data=block.train.dset.n_data, n_lbl=block.n_lbl)
        # data_repr, lbl_repr = model.data_repr.weight.data, model.lbl_repr.weight.data

        """
        pred_idx, pred_ptr, pred_score = get_predictions(data_repr, lbl_repr)

        indptr = block.train.dset.data.data_lbl.indptr
        output = { 
                'pred_idx': pred_idx, 
                'pred_ptr': pred_ptr, 
                'pred_score': pred_score, 
                'targ_idx': torch.from_numpy(block.train.dset.data.data_lbl.indices), 
                'targ_ptr': torch.from_numpy(np.stack([indptr[i+1]-indptr[i] for i in range(len(indptr)-1)])),
                }
        print(metric(**output))
        exit()
        """

        model = TCH001(DistilBertConfig(), n_data=block.train.dset.n_data, n_lbl=block.n_lbl)
        model.init_embeddings(data_repr, lbl_repr)
        # model.save_pretrained(f'{output_dir}/teacher')

    """ Metrics """
    if show_metrics or save_predictions:
        o = learn.predict(block.test.dset)
        
        if show_metrics: print(o.metrics)
        if save_predictions:
            pred_file = f'{output_dir}/predictions/test_predictions.pkl'
            os.makedirs(f'{output_dir}/predictions/', exist_ok=True)
            with open(pred_file, 'wb') as file: pickle.dump(o, file)
    
