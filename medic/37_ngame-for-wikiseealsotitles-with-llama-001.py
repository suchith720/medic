# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/37_ngame-for-wikiseealsotitles-with-llama.ipynb.

# %% auto 0
__all__ = ['prompt_func_1', 'prompt_func_2']

# %% ../nbs/37_ngame-for-wikiseealsotitles-with-llama.ipynb 2
import os,torch, torch.multiprocessing as mp, pickle, numpy as np
from xcai.basics import *
from xcai.models.LLL0XX import LAM009

from peft import (
    LoraConfig, 
    prepare_model_for_kbit_training,
    get_peft_model, 
    TaskType,
    PeftModel
)

# %% ../nbs/37_ngame-for-wikiseealsotitles-with-llama.ipynb 4
os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5,6,7,8,9'
os.environ['WANDB_PROJECT']='llama_00-wikiseealsotitles'

# %% ../nbs/37_ngame-for-wikiseealsotitles-with-llama.ipynb 7
def prompt_func_1(text):
    prompt = f"""
    Given the title of a wikipedia article and the corresponding categories of that article on wikipedia,
    your task is to predict the titles of all articles which are likely to be listed in the see also section of the
    mentioned article. Output the coma separated list of titles of the articles in the see also section of the
    given article.
    ### Input : 
    ### Title : {text}
    
    #### Task Output :
    #### Predicted title :
    """
    return prompt
    

# %% ../nbs/37_ngame-for-wikiseealsotitles-with-llama.ipynb 9
def prompt_func_2(text):
    prompt = f"""
    Given the title of a wikipedia article and the corresponding categories of that article on wikipedia,
    your task is to predict the titles of all articles which are likely to be listed in the see also section of the
    mentioned article. Output the coma separated list of titles of the articles in the see also section of the
    given article. Here are some samples:
    ### Title : Kaginele
    #### Ground truth title : Karnataka, Kanakagiri, Hampi, Haveri, Byadagi
    
    ### Title : krama
    #### Ground truth title : Tagelmust, Keffiyeh, Agal, Gingham, Turban
    
    ### Title: Triangulation (chess)
    #### Ground truth title : Corresponding squares, Opposition, Zugzwang, Key square, Tempo(chess)
    
    ### Input : 
    ### Title : {text} 
    
    #### Task Output :
    #### Predicted title :
    """
    return prompt
    

# %% ../nbs/37_ngame-for-wikiseealsotitles-with-llama.ipynb 32
if __name__ == '__main__':
    build_block = False 

    data_dir = '/data/datasets'
    pkl_dir = '/home/aiscuser/scratch1/datasets/'
    
    output_dir='/home/aiscuser/scratch1/outputs/medic/37_ngame-for-wikiseealsotitles-with-llama-001'

    """ Load data """
    pkl_file = f'{pkl_dir}/processed/wikiseealsotitles_data_meta-llama-3-8b_oak_32.pkl'

    if build_block:
        from transformers import AutoTokenizer
        tokz = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')
        tokz.add_special_tokens({"pad_token": "<PAD>"})

        block = XCBlock.from_cfg(data_dir, 'data', transform_type='oak', tokenizer=tokz, metadata_name='lnk', num_labels=4, num_metadata=3,
                                 max_sequence_length=32, padding=True, return_tensors='pt')

        with open(pkl_file, 'wb') as file: pickle.dump(block, file)
    else:
        with open(pkl_file, 'rb') as file: block = pickle.load(file)


    """ Training Arguements """
    args = XCLearningArguments(
        output_dir=output_dir,
        logging_first_step=True,
        per_device_train_batch_size=200,
        per_device_eval_batch_size=200,
        representation_num_beams=200,
        representation_accumulation_steps=10,
        save_strategy="steps",
        evaluation_strategy="steps",
        eval_steps=5000,
        save_steps=5000,
        save_total_limit=5,
        num_train_epochs=300,
        predict_with_representation=True,
        representation_search_type='BRUTEFORCE',
        adam_epsilon=1e-6,
        warmup_steps=100,
        weight_decay=0.01,
        learning_rate=2e-4,
        
        group_by_cluster=True,
        num_clustering_warmup_epochs=10,
        num_cluster_update_epochs=5,
        num_cluster_size_update_epochs=25,
        clustering_type='EXPO',
        minimum_cluster_size=2,
        maximum_cluster_size=1600,
        
        metric_for_best_model='P@1',
        load_best_model_at_end=True,
        target_indices_key='plbl2data_idx',
        target_pointer_key='plbl2data_data2ptr',
        
        use_encoder_parallel=True,
        max_grad_norm=None,
        # fp16=True,

        # accelerator_config={"use_configured_state":True},

        use_cpu_for_searching=True,
        use_cpu_for_clustering=True,

        clustering_devices=[2,3,4,5],
    )

    metric = PrecRecl(block.n_lbl, block.test.data_lbl_filterer, prop=block.train.dset.data.data_lbl,
                      pk=10, rk=200, rep_pk=[1, 3, 5, 10], rep_rk=[10, 100, 200])

    """ Model """
    bsz = max(args.per_device_train_batch_size, args.per_device_eval_batch_size)*torch.cuda.device_count()
    model = LAM009.from_pretrained('meta-llama/Meta-Llama-3-8B', bsz=bsz, tn_targ=5000, margin=0.3, tau=0.1, n_negatives=10, 
                                   apply_softmax=True, use_encoder_parallel=True, load_in_8bit=True)
    
    model.init_retrieval_head()
    vocab_size = model.encoder.embed_tokens.num_embeddings
    model.encoder.resize_token_embeddings(vocab_size+1)

    lora_config = LoraConfig(
        r=8,
        lora_alpha=8,
        lora_dropout=0.1,
        target_modules=["q_proj", "k_proj","v_proj","o_proj"],
        bias='none',
    )
    model = prepare_model_for_kbit_training(model)
    peft_model = get_peft_model(model, lora_config)
    peft_model.base_model.encoder.dr_head.requires_grad_(True)
    
    learn = XCLearner(
        model=peft_model, 
        args=args,
        train_dataset=block.train.dset,
        eval_dataset=block.test.dset,
        data_collator=block.collator,
        compute_metrics=metric,
    )
    
    mp.freeze_support()
    learn.train()
    
